{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP 2 - Echantillonnage compressif\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <Font color = \"blue\"> ENGELSEN Gorm et RAHAL Skander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.fftpack as fft\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as npl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récemment (début années 2004-présent), de nouveaux concepts et théorèmes ont été développés et risquent de \n",
    "révolutionner à relativement court terme la fabrication de certains appareils de mesure numériques (microphones, imageurs, analyseurs de spectres,...). \n",
    "Ces nouvelles techniques sont couramment appelées échantillonnage compressif, \"compressive sampling\" ou encore \"compressed sensing\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Le théorème de Shannon"
   ]
  },
  {
   "attachments": {
    "EchantillonageSous_Nyquist.png": {
     "image/png": "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"
    },
    "Echantillonnage09.png": {
     "image/png": "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"
    },
    "Echantillonnage105.png": {
     "image/png": "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"
    },
    "Echantillonnage201.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aujourd'hui, presque tous les appareils de mesure reposent sur le théorème de Shannon. Celui-ci (vous l'avez déjà vu en 2ème année) peut s'énoncer ainsi : \n",
    "> Soit $g:\\mathbb{R}\\to \\mathbb{R}$ une fonction de $L^2(\\mathbb{R})$. Si sa transformée de Fourier $\\hat g$ a un support contenu dans l'intervalle $[-f_M, f_M]$, alors en l'échantillonnant à une fréquence d'échantillonnage $f_e\\geq 2f_M$, on peut la reconstruire exactement.\n",
    "\n",
    "Ce théorème est illustré sur les figures ci-après:\n",
    "![Echantillonnage105.png](attachment:Echantillonnage105.png)![Echantillonnage201.png](attachment:Echantillonnage201.png)\n",
    "![Echantillonnage09.png](attachment:Echantillonnage09.png)![EchantillonageSous_Nyquist.png](attachment:EchantillonageSous_Nyquist.png)\n",
    "\n",
    "Les instruments de mesures qui reposent sur ce théorème sont donc construits suivant le principe : \n",
    ">Filtre passe-bas $\\rightarrow$ Echantillonnage à une fréquence $f>2f_M$ $\\rightarrow$ Interpolation sinc\n",
    "\n",
    "Pour beaucoup d'applications, ce principe présente deux défauts majeurs :\n",
    "* Les signaux sont rarement naturellement à spectre borné, et on perd donc l'information haute-fréquence en effectuant un filtrage passe-bas.\n",
    "* Pour beaucoup de signaux, il faut choisir une très haute fréquence d'échantillonnage pour obtenir un résultat satisfaisant. \n",
    "Ceci implique que les données à stocker ont une taille très importante et qu'il faut les compresser après coup (par exemple : jpeg).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L'échantillonnage compressif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Principe général**\n",
    "\n",
    "L'idée sous jacente à l'échantillonnage compressif est de réaliser la compression dès l'acquisition.\n",
    "Supposons que le signal $x\\in \\mathbb{R}^n$ que l'on souhaite mesurer s'écrive comme une combinaison linéaire de la forme :\n",
    "\\begin{equation}\n",
    "(1)~~~~~~~~~~~ x=\\sum_{i=1}^m\\alpha_i \\psi_i\n",
    "\\end{equation}\n",
    "où $\\psi_i\\in \\mathbb{R}^n, \\ i=1..m$, sont des \"fonctions de base\" (en traitement d'images, ces fonctions pourraient être des ondelettes, en traitement du son, des ondelettes ou des atomes de Fourier, pour certaines applications, on pourrait imaginer des splines...} et $\\alpha_i\\in \\mathbb{R}$ sont des coefficients. \n",
    "On peut réécrire l'équation (1) sous la forme matricielle condensée :\n",
    "$$\n",
    "x=\\Psi \\alpha \\ \\ \\textrm{où } \\ \\ \\alpha=\\begin{pmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_m \\end{pmatrix}\\ \\ \\textrm{et} \\ \\ \\Psi=\\begin{pmatrix} \\psi_1,\\psi_2,..., \\psi_m\\end{pmatrix}.\n",
    "$$\n",
    "Pour pouvoir reconstruire tous les éléments de $\\mathbb{R}^n$, on suppose généralement que la matrice $\\Psi$ est une matrice surjective (ainsi, la famille  des $(\\Psi_i)_i$ est génératrice), ce qui implique que $m\\geq n$. Dans le langage du traitement d'image, on dit alors que $\\Psi$ est un frame (une base si $m=n$).\n",
    "\n",
    "L'échantillonnage compressif repose sur l'hypothèse suivante : les signaux $x$ que l'on souhaite mesurer sont parcimonieux, \n",
    "c'est-à-dire que la majorité des coefficients $\\alpha_i$ dans (1) sont nuls ou encore que \n",
    "$$\\#\\{\\alpha_i\\neq 0, i=1..m\\}\\ll n.$$\n",
    "On va voir que cette hypothèse permet - dans certains cas - de réduire drastiquement le nombre de mesures par rapport au théorème de Shannon avec en contre-partie, le besoin de résoudre un problème d'optimisation pour reconstruire la donnée. L'objectif de ce TP est de résoudre le problème d'optimisation résultant.\n",
    "\n",
    "Le principe de l'acquisition du signal $x$ est le suivant :\n",
    "\n",
    "- On effectue un petit nombre $p\\ll n$ de mesures linéaires du signal $x$ inconnu. On note ces mesures $y_i$, et comme elles sont linéaires par rapport à $x$, il existe pour chaque $i$ un vecteur $a_i\\in \\mathbb{R}^n$ tel que \n",
    "$$y_i=\\langle a_i, x\\rangle, i=1..p.$$ On peut aussi écrire cette opération de mesure sous la forme condensée :\n",
    "$$\n",
    "y=Ax\\ \\ \\textrm{où } \\ \\ y=\\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_p\\end{pmatrix} \\ \\ \\textrm{et} \\ \\ A=\\begin{pmatrix} a_1^T\\\\a_2^T\\\\ \\vdots\n",
    "\\\\ a_p^T\\end{pmatrix}.\n",
    "$$\n",
    "- On reconstruit le signal $x$ en résolvant le problème contraint suivant :\n",
    "\n",
    "$$\n",
    "(2)~~~~~~~~~~~ \\mbox{Trouver } \\alpha^\\star \\mbox{ solution de: }\\displaystyle\\min_{\\alpha \\in \\mathbb{R}^m, A\\Psi\\alpha=y} \\|\\alpha\\|_0\n",
    "$$\n",
    "\n",
    "où $\\|\\cdot\\|_0$ est la norme de comptage, aussi appelée norme $l^0$ définie par : \n",
    "$$\n",
    "\\|\\alpha\\|_0=\\#\\{\\alpha_i\\neq 0, i=1..m\\}.\n",
    "$$\n",
    "Autrement dit, l'idée est la suivante : on  cherche $\\alpha^\\star$, le signal le plus parcimonieux dans le frame $\\Psi$, parmi les signaux qui peuvent donner lieu aux mesures $y$. \n",
    "Après avoir trouvé $\\alpha^\\star$, on recouvre $\\tilde x$, une approximation du signal $x$ en calculant $\\tilde x=\\Psi\\alpha^\\star$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Simplification du problème d'optimisation**\n",
    "\n",
    "Le problème précédent est un problème combinatoire NP-complet, ce qui signifie que trouver $\\alpha$ peut demander un temps exponentiel en fonction de $n$, la dimension du signal. Pour le résoudre en pratique, il est souvent remplacé par : \n",
    "\n",
    "$$\n",
    "(3)~~~~~~~~~~~  \\mbox{Trouver } \\alpha^*\\in \\displaystyle\\arg\\min_{\\alpha \\in \\mathbb{R}^m, A\\Psi\\alpha=y} \\|\\alpha\\|_1\n",
    "$$\n",
    "\n",
    "où $\\|\\alpha\\|_1=\\sum_{i=1}^m|\\alpha_i|$ est la norme $l^1$ de $\\alpha$. On peut dans certains cas montrer que les solutions de (2) et de (3) sont identiques. \n",
    "\n",
    "Un appareil de mesure n'étant jamais parfait, il est impossible de mesurer exactement $y_i=\\langle a_i, x\\rangle$. \n",
    "Le vecteur $y$ est bruité et la contrainte $A\\Psi\\alpha=y$ est trop forte. Elle est donc généralement relaxée et le problème devient : \n",
    "\n",
    ">$$(4)~~~~~~~~~~~  \\mbox{Trouver } \\alpha^*\\in \\arg\\min_{\\alpha \\in \\mathbb{R}^m} \\|\\alpha\\|_1 + \\frac{\\sigma}{2} \\|A\\Psi\\alpha-y\\|_2^2.$$\n",
    "\n",
    "Si $\\sigma$ tend vers $0$, la solution du problème (4) tend vers une solution du problème (3). C'est le problème (4) que nous allons résoudre dans ce TP. Dans la suite , on notera $F$ la fonction :\n",
    "$$\n",
    "F(\\alpha)=\\|\\alpha\\|_1 + \\frac{\\sigma}{2} \\|A\\Psi\\alpha-y\\|_2^2.\n",
    "$$\n",
    "\n",
    "On peut remarquer que les problèmes (3) et (4) sont convexes (contraintes convexes et fonctions convexes) tandis que le problème (2) ne l'est pas. En revanche, aucun des trois problèmes n'est différentiable.\n",
    "\n",
    "Pour conclure cette introduction à l'échantillonnage compressif, notons que de façon similaire au théorème de Shannon, on dispose d'une condition de reconstruction exacte :\n",
    "\n",
    "> Supposons que :\n",
    "* $x=\\displaystyle\\sum_{i=1}^m\\alpha_i\\psi_i\\in \\mathbb{R}^n$ avec $\\|\\alpha\\|_0=k$.\n",
    "* On effectue $p$ mesures linéaires de $x$ avec $p\\geq C \\cdot k \\cdot \\log(n)$, où $C=20$.\n",
    "* On choisit les coefficients de la matrice $A\\in \\mathcal{M}_{p,n}$ de façon **aléatoire** (e.g. on peut choisir les coefficients $a_{i,j}$ de $A$ de façon indépendante suivant une loi normale.)\n",
    "\n",
    "> Alors, la résolution du problème (3) permet de reconstruire $x$ **exactement** avec une très grande probabilité \n",
    "\n",
    "L'expérience a montré qu'en pratique, il suffit en général de $p=2k$ mesures pour reconstruire le signal exactement en grande dimension !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Algorithme Forward Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commencons par quelques rappels: soit $F=f+g$ une fonction convexe composite avec $f$ différentiable à gradient \n",
    "$L$-Lipschitz et $g$ une fonction convexe dont on sait calculer l'opérateur proximal. L'algorithme Forward-Backward s'écrit alors:\n",
    "$$x_{k+1}=prox_{sg}(x_k-s\\nabla f(x_k)).$$\n",
    "\n",
    "On peut montrer que la suite $(F(x_k)-F(x^*))_{k\\in \\mathbb N}$ est décroissante et: \n",
    "$$F(x_k)-F(x^*)\\leqslant \\frac{2\\Vert x_0-x^*\\Vert^2}{sk}$$\n",
    "Cette vitesse en $\\frac{1}{k}$ est optimale au sens où il n'est pas possible de trouver des bornes qui décroissent en $\\frac{1}{k^{\\delta}}$ avec $\\delta>1$ pour toutes les fonctions convexes. On peut montrer que si $s<\\frac{1}{L}$ on a en fait \n",
    "$$F(x_k)-F(x^*)=o\\left(\\frac{1}{k}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revenons maintenant au problème (4) et appliquons l'algorithme FB pour le résoudre. Pour cela: \n",
    "\n",
    "**Q1.** Soit $J(\\alpha) =\\frac{\\sigma}{2}\\|A\\Psi \\alpha - y\\|_2^2$. Calculez $\\nabla J(\\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "J(\\alpha) &=\\frac{\\sigma}{2}\\|A\\Psi \\alpha - y\\|_2^2\\\\\n",
    "&= \\frac{\\sigma}{2}(A\\Psi \\alpha - y)^T(A\\Psi \\alpha - y)\\\\\n",
    "&= \\frac{\\sigma}{2}(A^T\\Psi^T \\alpha^T - y^T)^T(A\\Psi \\alpha - y)\\\\\n",
    "&= \\frac{\\sigma}{2}(\\alpha^T\\Psi^TA^TA\\Psi\\alpha - \\alpha^T\\Psi^TA^Ty - y^TA\\Psi\\alpha + y^Ty)\\\\\n",
    "\\nabla J (\\alpha) &= \\sigma (A \\Psi)^T (A \\Psi \\alpha - y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Montrer que la fonction $J$ est de classe $C^1$ à gradient Lipschitz et calculer un majorant $L$ de la constante de Lipschitz de $\\nabla J$ en fonction de $|||A|||$, de $|||\\Psi |||$ et de $\\sigma$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $J$ est de classe $C^1$:\n",
    "\n",
    "$ A\\Psi \\alpha - y $ est linaire et $ ||\\cdot||^2 $ est une norme carré donc $J(\\alpha)$ est de classe $C^1$\n",
    "\n",
    "#### Gradent Lipschitz:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "||\\nabla J(\\alpha) - \\nabla J(\\beta)|| &= ||\\sigma(A\\Psi)^T(A\\Psi\\alpha-y)-\\sigma(A\\Psi)^T(A\\Psi\\beta-y)||\\\\\n",
    "&= \\sigma||(A \\Psi)^T A \\Psi (\\alpha - \\beta)||\\\\\n",
    "&=\\leq |||(A \\Psi)^T A \\Psi|||\\cdot||\\alpha - \\beta||\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donc $\\nabla (J)$ est Lipschitz avec $L = \\sigma |||(A \\Psi)^T A \\Psi|||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** On note $\\alpha^k$ l'itéré courant. On veut utiliser l'algorithme Forward-Backward pour la résolution du problème (4) en choisissant un pas constant égal à $\\frac{1}{L}$. Ecrire une itération de l'algorithme FB et donner la formule analytique permettant de calculer\n",
    "$\\alpha^{k+1}$ en fonction de $\\alpha^k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\alpha^{k+1} &= \\mbox{prox}_{\\frac{1}{L}\\left\\|.\\right\\|_1}\\Big(\\alpha^k-\\frac{1}{L}\\nabla J(\\alpha^k)\\Big) \\\\\n",
    "&= \\mbox{prox}_{\\frac{1}{L}\\left\\|.\\right\\|_1}\\Big(\\alpha^k-\\frac{1}{L}\\sigma (A \\Psi)^T (A \\Psi \\alpha^k - y)\\Big)\\\\\n",
    "&=sgn\\Big(\\alpha^k-\\frac{1}{L}\\sigma (A \\Psi)^T (A \\Psi \\alpha^k - y)\\Big)\\cdot max \\Big(\\alpha^k-\\frac{1}{L}\\sigma (A \\Psi)^T (A \\Psi \\alpha^k - y) - \\frac{1}{L}, 0\\Big)\n",
    "\\end{align}\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Q4.** De quelle quantité la fonction coût $F(\\alpha)$ décroît-elle à chaque itération ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche trouver $F(\\alpha^k) - F(\\alpha^{k+1})$ ou \n",
    "$$F(\\alpha) = \\| \\alpha \\|_1 + \\frac{\\sigma}{2}\\|A\\Psi \\alpha - y\\|_2^2 = \\| \\alpha \\|_1 + J(\\alpha).$$\n",
    "On a que une iteration de $\\alpha_k$ est:\n",
    "$$\\alpha^{k+1} = \\mbox{prox}_{\\frac{1}{L}\\left\\|.\\right\\|_1}\\Big(\\alpha^k-\\frac{1}{L}\\nabla J(\\alpha^k)\\Big) = \\arg\\min_{y \\in \\mathbb{R}^m}(\\frac{1}{L}||y||_1+\\frac{1}{2}||y-\\alpha^k+\\frac{1}{L}\\nabla J(\\alpha^k)||_2^2)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a un problème d'optimisation non différentiable et convexe. La condition nécessaire et suffisante d'optimalité est alors:\n",
    "$$\n",
    "0\\in\\frac{1}{L}\\partial_{||.||_1}(\\alpha^{k+1})+\\alpha^{k+1}-\\alpha^k+\\frac{1}{L}\\nabla J(\\alpha^k) \\\\ \\alpha^k - \\alpha^{k+1} - \\frac{1}{L}\\nabla J(\\alpha^k)\\in \\frac{1}{L}\\partial_{||.||_1}(\\alpha^{k+1}) $$\n",
    "\n",
    "\n",
    "On a par définition du sous-gradient\n",
    "$$\n",
    "\\begin{align} \\frac{1}{L}||\\alpha^{k}||_1 &\\geq  \\frac{1}{L}||\\alpha^{k+1}||_1 + <\\alpha^k - \\alpha^{k+1}-\\frac{1}{L}\\nabla J(\\alpha^{k}), \\alpha^{k}-\\alpha^{k+1}>\\\\\n",
    " \\frac{1}{L}(||\\alpha^{k}||_1-||\\alpha^{k+1}||_1 ) &\\geq   <\\alpha^k - \\alpha^{k+1}, \\alpha^{k}-\\alpha^{k+1}> -\\frac{1}{L}<\\nabla J(\\alpha^{k}), \\alpha^{k}-\\alpha^{k+1}> \\\\\n",
    " ||\\alpha^{k}||_1-||\\alpha^{k+1}||_1  &\\geq  L ||\\alpha^k - \\alpha^{k+1}||_2^2 -<\\nabla J(\\alpha^{k}), \\alpha^{k}-\\alpha^{k+1}>\\end{align}$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque $J$ est à gradient L-Lipschitz on a que:\n",
    "\n",
    " $$J(y) - J(x)  \\leq <\\nabla J(x), y-x> + \\frac{L}{2}||x - y||_2^2 $$\n",
    "\n",
    "Si on pose que $y=\\alpha^{k+1}$ et $x=\\alpha^{k}$ on obtient que:\n",
    "\n",
    " $$- <\\nabla J(\\alpha^k), \\alpha^k-\\alpha^{k+1}> \\geq J(\\alpha^{k+1}) - J(\\alpha^k) - \\frac{L}{2}||\\alpha^k - \\alpha^{k+1}||_2^2 $$\n",
    " \n",
    "Et on peut utiliser cette equation dans l'expression au dessus pour obtenir\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "||\\alpha^{k}||_1-||\\alpha^{k+1}||_1  &\\geq  L ||\\alpha^k - \\alpha^{k+1}||_2^2 +J(\\alpha^{k+1}) - J(\\alpha^k) - \\frac{L}{2}||\\alpha^k - \\alpha^{k+1}||_2^2 \\\\\n",
    "||\\alpha^{k}||_1 + J(\\alpha^k)-||\\alpha^{k+1}||_1 - J(\\alpha^{k+1})  &\\geq  L ||\\alpha^k - \\alpha^{k+1}||_2^2 - \\frac{L}{2}||\\alpha^k - \\alpha^{k+1}||_2^2\\\\\n",
    " F(\\alpha^k) - F(\\alpha^{k+1}) &\\geq \\frac{L}{2}||\\alpha^k - \\alpha^{k+1}||_2^2\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "Donc la fonction coût 𝐹(𝛼) décroît d'au moins $\\frac{L}{2}||\\alpha^k - \\alpha^{k+1}||_2^2$ à chaque itération.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Partie expérimentale. Les données\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, on va chercher à reconstruire un signal unidimensionnel $x~:~[0,1]~\\rightarrow~\\mathbb{R}$ de la forme :\n",
    "$$\n",
    "x(t)= \\alpha_{k}\\delta_{k/n}(t)+\\sum_{k=1}^n\\alpha_{k+n}\\cos\\left(\\frac{2k\\pi}{n} t\\right)\n",
    "$$ \n",
    "on a donc $m=2n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialisations\n",
    "n=500            #Taille de l'echantillon\n",
    "t=np.linspace(0,1,n) #On definit un signal sur [0,1]\n",
    "\n",
    "## Generation du signal\n",
    "x=np.zeros(n)\n",
    "tmp=np.zeros(n)\n",
    "#On ajoute deux cosinus\n",
    "tmp[350]=4\n",
    "x+=fft.idct(tmp,norm='ortho')  \n",
    "tmp=np.zeros(n)\n",
    "tmp[150]=-3  \n",
    "x+=fft.idct(tmp,norm='ortho')\n",
    "#On ajoute deux diracs\n",
    "x[int(n/3)]=0.2;    #Tester 0.5\n",
    "x[int(2*n/3)]=-0.3; #Tester -1\n",
    "\n",
    "plt.plot(t,x,'b')\n",
    "plt.show()\n",
    "## Mesure du signal\n",
    "p=20*4       #Nombre de mesures\n",
    "A=np.random.randn(p,n) #La matrice de mesure\n",
    "y=A.dot(x)        #Les mesures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code *Generesignal.py* génère un signal discret $x$ qui peut être vu comme une combinaison linéaire de cosinus à différentes fréquences et de diracs. Ce signal n'est pas parcimonieux dans la base canonique des diracs (car il faut à peu près $n$ diracs pour représenter un cosinus) et il n'est pas parcimonieux dans la base des sinus (il faut faire une combinaison linéaire de $n$ cosinus pour représenter un dirac).\n",
    "\n",
    "Par contre, ce signal est parcimonieux dans un frame qui est l'union de la base canonique et de la base des cosinus. \n",
    "Dans ce frame, il suffit en effet de $4$ coefficients non nuls pour reconstruire parfaitement le signal.\n",
    "\n",
    "> On choisira donc le frame représenté par une matrice $\\Psi=(I,C) \\in \\mathcal{ M}_{2n,n}(\\mathbb{R})$ o\\`u $C$ est une base de cosinus à différentes fréquences.\n",
    "\n",
    "#### 4.2. Implémentation de l'itération proximale\n",
    "\n",
    "**Q5** Implémentez l'opérateur linéaire $\\Psi$ et son adjoint $\\Psi^*$. \n",
    "\n",
    "Pour $\\Psi$, vous vous servirez de la fonction $dct$ de Python dans la libraire scipy.fftpack qui calcule la transformée en cosinus discret d'un vecteur. Vous ferez attention à préciser *norm='ortho'* dans les options de la $dct$ pour que $idct$ soit bien l'opération inverse de $dct$.\n",
    "\n",
    "Pour $\\Psi^*$, vous utiliserez le fait que la $dct$ est une isométrie quand on précise \\textit{norm='ortho'} dans les options de $dct$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear function Psi\n",
    "## (combination of sines and diracs) \n",
    "def Psi(alpha) :\n",
    "    n = int(len(alpha)/2)\n",
    "    C = fft.idct(alpha[n:], norm='ortho') \n",
    "    I = alpha[:n]\n",
    "    return I + C\n",
    "\n",
    "## The transpose of Psi\n",
    "def PsiT(x) :\n",
    "    n = len(x)\n",
    "    alpha = np.zeros(2*n)\n",
    "    alpha[:n] = x\n",
    "    alpha[n:] = fft.dct(x, norm='ortho')\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6** Implémentez l'algorithme proximal dans la fonction *RestoreX* avec les notations suivantes:\n",
    "* $A$ est la matrice d'échantillonnage.\n",
    "* $y$ est le vecteur de mesures.\n",
    "* $sigma$ est un paramètre du modèle.\n",
    "* $nit$ est le nombre d'itérations.\n",
    "* $alpha$ est la solution approximative du problème (4).\n",
    "* $x$ est donné par Psi $(\\alpha)$.\n",
    "* $CF$ est la fonction coût à chaque itération de l'algorithme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prox of the l1−norm \n",
    "def prox(alpha,gamma) :\n",
    "    beta = np.sign(alpha)*np.maximum(np.abs(alpha) - gamma, 0)\n",
    "    return beta\n",
    "\n",
    "def cost_function(alpha, sigma, A, y):\n",
    "    return npl.norm(alpha,1) + sigma/2*npl.norm(A.dot(Psi(alpha))-y,2)**2\n",
    "\n",
    "\n",
    "def RestoreX(A,y,sigma,nit):\n",
    "    t0 = time.time()\n",
    "    n = np.shape(A)[1]\n",
    "    x = np.zeros(n)\n",
    "    alpha = np.zeros(2*n)\n",
    "    alpha_arr = []\n",
    "    CF = []\n",
    "    L = 2*sigma*npl.norm(A.T@A,2)\n",
    "    gamma = 1/L\n",
    "    \n",
    "    for i in range(nit):\n",
    "        gradJ = sigma * PsiT((A.T).dot(A@Psi(alpha) - y))\n",
    "        x_k = alpha - gamma*gradJ\n",
    "        alpha = prox(alpha - gamma*gradJ, gamma)\n",
    "        CF.append(cost_function(alpha, sigma, A, y))\n",
    "        alpha_arr.append(alpha)\n",
    "        if (i > 1):\n",
    "            if (max(abs(alpha_arr[i] - alpha_arr[i-1])) < 10e-8):\n",
    "                x = Psi(alpha)\n",
    "                t = time.time() - t0\n",
    "                return alpha,x,np.array(CF), alpha_arr, t, i+1\n",
    "\n",
    "    x = Psi(alpha)\n",
    "    t = time.time()-t0\n",
    "    return alpha,x,np.array(CF), alpha_arr, t, i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Testez votre algorithme ! Les paramètres $sigma$ et $nit$ sont des à choisir par vous-même (il faut en pratique beaucoup d'itérations pour converger). Vous pourrez observer la façon dont la suite $\\alpha^k$ se comporte au fur et à mesure des itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "nit = 10000\n",
    "alpha,xtilde,CF, alpha_k, runtime , it = RestoreX(A,y,sigma,nit)\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.where(alpha != 0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,10))\n",
    "ax[0].set_title(\"Alpha coefficients\")\n",
    "ax[0].plot(alpha)\n",
    "ax[1].set_title(\"Alpha_k coefficients\")\n",
    "ax[1].plot(alpha_k)\n",
    "plt.show()\n",
    "\n",
    "print(\"Alpha coefficients: \", str(l[0].flatten()))\n",
    "a_v = []\n",
    "for i in l[0].flatten():\n",
    "    a_v.append(round(alpha[i],3))\n",
    "    \n",
    "\n",
    "print(\"Alpha values: \", str(a_v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Commentaire\n",
    "On peut facilement voir que les 4 coefficients non nuls sont $166, 333, 650$ et $850$.\n",
    "A droite, on peut constater la convergence de coefficients de $\\alpha$.Cela nécessite environ 4500 itérations à l'algorithme pour qu'il puisse converger.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nit = 10000\n",
    "Sigma = [0.1, 1, 10, 100]\n",
    "alpha_1,xtilde_1,CF_1, alpha_k_1, t1, it1=RestoreX(A,y,Sigma[0],nit)\n",
    "alpha_2,xtilde_2,CF_2, alpha_k_2, t2, it2=RestoreX(A,y,Sigma[1],nit)\n",
    "alpha_3,xtilde_3,CF_3, alpha_k_3, t3, it3=RestoreX(A,y,Sigma[2],nit)\n",
    "alpha_4,xtilde_4,CF_4, alpha_k_4, t4, it4=RestoreX(A,y,Sigma[3],nit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,3, figsize = (12,17))\n",
    "fig.suptitle(\"Comparison of different sigma\", size = 20)\n",
    "ax[0][0].plot(alpha_1)\n",
    "ax[0][0].set_title(\"Alpha coefficients with $\\sigma = $\" + str(Sigma[0]))\n",
    "ax[0][1].plot(np.log(CF_1))\n",
    "ax[0][1].set_title(\"Log(CF) with $\\sigma = $\" + str(Sigma[0]))\n",
    "ax[0][1].grid()\n",
    "ax[0][2].plot(alpha_k_1)\n",
    "ax[0][2].set_title(\"Convergence of alpha with $\\sigma = $\" + str(Sigma[0]))\n",
    "\n",
    "ax[1][0].plot(alpha_2)\n",
    "ax[1][0].set_title(\"Alpha coefficients with $\\sigma = $\"  + str(Sigma[1]))\n",
    "ax[1][1].plot(np.log(CF_2))\n",
    "ax[1][1].set_title(\"Log(CF) with $\\sigma = $\"  + str(Sigma[1]))\n",
    "ax[1][1].grid()\n",
    "ax[1][2].plot(alpha_k_2)\n",
    "ax[1][2].set_title(\"Convergence of alpha with $\\sigma = $\" + str(Sigma[1]))\n",
    "\n",
    "ax[2][0].plot(alpha_3)\n",
    "ax[2][0].set_title(\"Alpha coefficients with $\\sigma = $\"  + str(Sigma[2]))\n",
    "ax[2][1].plot(np.log(CF_3))\n",
    "ax[2][1].set_title(\"Log(CF) with $\\sigma = $\"  + str(Sigma[2]))\n",
    "ax[2][1].grid()\n",
    "ax[2][2].plot(alpha_k_3)\n",
    "ax[2][2].set_title(\"Convergence of alpha with $\\sigma = $\" + str(Sigma[2]))\n",
    "\n",
    "ax[3][0].plot(alpha_4)\n",
    "ax[3][0].set_title(\"Alpha coefficients with $\\sigma = $\"  + str(Sigma[3]))\n",
    "ax[3][1].plot(np.log(CF_4))\n",
    "ax[3][1].set_title(\"Log(CF) with $\\sigma = $\"  + str(Sigma[3]))\n",
    "ax[3][1].grid()\n",
    "ax[3][2].plot(alpha_k_4)\n",
    "ax[3][2].set_title(\"Convergence of alpha with $\\sigma = $\" + str(Sigma[3]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "On voit que pour $\\sigma$ trop petit, on risque de sauter des coefficients non nuls et pour $\\sigma$ trop grand , on constate que l'algorithme nécessite beaucoup de temps avant de converger. Donc on choisit $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** Vérifiez que la fonction coût décroit de façon monotone. Quel est le taux de convergence observé ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 2\n",
    "nit = 10000\n",
    "alpha,xtilde,CF, alpha_k , t , it= RestoreX(A,y,sigma,nit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = np.zeros(len(CF)-1)\n",
    "for i in range(len(CF)-1):\n",
    "    dec[i] = CF[i]-CF[i+1]\n",
    "print(\"Minimum value:\" + str(min(dec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur minimale de la séquence avec la différence de chaque itération est strictement positive. La fonction coût décroit de façon monotone.\n",
    "\n",
    "Ci-dessous on a créé une fonction pour calculer la taux de convergence observé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence_rate(x, L):\n",
    "    n = len(x[:-1])\n",
    "    diff = np.abs(x[:-1] - L)\n",
    "    ratio = diff[1:] / diff[:-1]\n",
    "    log_ratio = np.log(ratio)\n",
    "    slope, _ = np.polyfit(np.arange(n-1), log_ratio, 1)\n",
    "    rate = np.exp(slope)\n",
    "    return rate\n",
    "\n",
    "print(\"Taux de convergence:\", round(convergence_rate(CF,CF[-1]), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut donc verifier que le taux de convergence observé est en o( $\\frac{1}{k}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** A partir de combien de mesures pouvez-vous reconstruire exactement le signal $x$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nit = 10000\n",
    "n = 500\n",
    "sigma = 1\n",
    "p = [80, 40, 30, 20]\n",
    "\n",
    "A1=np.random.randn(p[0],n) \n",
    "y1=A1.dot(x)        \n",
    "alpha_1,xtilde_1,CF_1, alpha_k_1 , t1, it1 =RestoreX(A1,y1,sigma,nit)\n",
    "\n",
    "A2=np.random.randn(p[1],n) \n",
    "y2=A2.dot(x)        \n",
    "alpha_2,xtilde_2,CF_2, alpha_k_2, t2, it2  =RestoreX(A2,y2,sigma,nit)\n",
    "\n",
    "A3=np.random.randn(p[2],n) \n",
    "y3=A3.dot(x)        \n",
    "alpha_3,xtilde_3,CF_3, alpha_k_3, t3, it3  =RestoreX(A3,y3,sigma,nit)\n",
    "\n",
    "A4=np.random.randn(p[3],n) \n",
    "y4=A4.dot(x)        \n",
    "alpha_4,xtilde_4,CF_4, alpha_k_4, t4, it4  =RestoreX(A4,y4,sigma,nit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotter_2(x, x_tilde, alpha_k, p):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(x_tilde)\n",
    "    plt.title(\"Reconstruction of x with $p = $\" + str(p))\n",
    "    fig, ax = plt.subplots(1,2, figsize = (15,7))\n",
    "    ax[0].plot(np.abs(x-x_tilde))\n",
    "    ax[0].set_title(\" $ |x - x'|$ with $p = $\" + str(p))\n",
    "    ax[1].plot(alpha_k)\n",
    "    ax[1].set_title(\"Convergence of alpha with $p = $\" + str(p))\n",
    "    ax[1].grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plotter_2(x, xtilde_1, alpha_k_1, p[0])\n",
    "\n",
    "plotter_2(x, xtilde_2, alpha_k_2, p[1])\n",
    "\n",
    "plotter_2(x, xtilde_3, alpha_k_3, p[2])\n",
    "\n",
    "plotter_2(x, xtilde_4, alpha_k_4, p[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentaire:\n",
    "\n",
    "On voit que pour $ p \\geq 40 $ on peut reconstruire le signal exactement avec une erreur relativement faible.Pour une valeur de $p$ plus grande que 40, on ne peut pas effectivement diminuer l'erreur mais avec une valeur de $p$ inférieure à 40, l'erreur augmente drastiquement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Implémentation de l'itération proximale accelérée\n",
    "\n",
    "On n'a a aucun moment utilisé la convexité de la fonction $J$ pour définir l'algorithme proximal. Celui-ci est de fait sous-optimal et peut être nettement accéléré. Yuri Nesterov a proposé dans les années 1980 plusieurs méthodes permettant l'accélération de la descente de gradient explicite. L'accélération de la descente de gradient proposée Yurii Nesterov en 1984 et adapatée à FB sous le nom de FISTA (Fast Iterative Soft Shrinckage Algorithm) par Beck et Teboulle en 2009 est d'une mise en oeuvre très simple: considérons à nouveau la fonction composite $F=f+g$ à minimiser. L'algorithme FISTA s'écrit:\n",
    "\\begin{eqnarray*}\n",
    "y_k &=& x_k+\\alpha_k(x_k-x_{k-1})\\\\\n",
    "x_{k+1} &=& {\\rm prox}_{sg}(y_k-s\\nabla f(x_k))\n",
    "\\end{eqnarray*}\n",
    "\n",
    "avec un pas $s<\\frac{1}{L}$ et $\\alpha_k>0$. On parle de méthode inertielle car cette méthode utilise un terme dit de \"mémoire\" ou inertiel qui exploite la dernière direction de descente.\n",
    "\n",
    "Le choix original de Nesterov pour la suite $\\alpha_k$ est le suivant :\n",
    "\\begin{equation}\n",
    "\\alpha_k=\\frac{t_k-1}{t_{k+1}}\\text{ avec }t_1=1\\text{ et }t_{k+1}=\\frac{1+\\sqrt{1+t_k^2}}{2}\n",
    "\\end{equation}\n",
    "Pour ce choix on a \n",
    "$$F(x_k)-F(x^*)\\leqslant \\frac{2\\Vert x_0-x^*\\Vert^2}{sk^2}$$\n",
    "On peut prendre plus simplement $$\\alpha_k=\\frac{k-1}{k+2}$$ et dans ce cas, on a $F(x_k)-F(x^*)=o\\left(\\frac{1}{k^2}\\right)$ et on a convergence de la suite $(x_k)_{k\\geqslant 1}$. On peut noter que dans ce cas, la première étape est sans inertie ($\\alpha_1=0$) et donc $x_1=T(x_0)$. L'inertie apparait pour le calcul de $x_2$. \n",
    "\n",
    "A noter que la suite de terme général $F(x_k)-F(x^*)$ n'est pas nécessairemment décroissante comme dans le cas de FB ou de la descente de gradient. Dans la pratique vous verrez que FISTA est quand même plus rapide que FB.\n",
    "\n",
    " \n",
    "**Q10.** En vous aidant de ce que vous avez codé dans la partie précédente, implémentez cet algorithme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nesterov(A,y,sigma,nit):\n",
    "    t0 = time.time()\n",
    "    n = np.shape(A)[1]\n",
    "    x = np.zeros(n)\n",
    "    alpha = np.zeros(2*n)\n",
    "    alpha_p = np.zeros(2*n)\n",
    "    alpha_arr = []\n",
    "    CF = np.zeros(nit)\n",
    "    L = 2*sigma*npl.norm(A.T@A,2)\n",
    "    gamma = 1/L\n",
    "    for i in range(nit):\n",
    "        tk = (i-1)/(i+2)\n",
    "        \n",
    "        yk = alpha + tk*(alpha - alpha_p)\n",
    "        gradJ = sigma * PsiT((A.T).dot(A@Psi(yk) - y))\n",
    "        alpha_p = alpha\n",
    "        alpha = prox(yk - gamma*gradJ, gamma)\n",
    "        alpha_arr.append(alpha)\n",
    "        CF[i] = cost_function(alpha, sigma, A, y)\n",
    "        if (i > 1):\n",
    "            if (max(abs(alpha_arr[i] - alpha_arr[i-1])) < 10e-5):\n",
    "                x = Psi(alpha)\n",
    "                t = time.time() - t0\n",
    "                return alpha,x,CF, alpha_arr, t, i+1\n",
    "    x = Psi(alpha)\n",
    "    t = time.time() - t0\n",
    "    return alpha,x,CF, alpha_arr, t, i+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Testez le et comparez la rapidité d'execution de l'algorithme précédent et de celui-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter_RvsN(a_r_R, a_r_N, t_N, it_N, t_R, it_R):\n",
    "    \n",
    "    print(\"Runtime Nesterov:    \" + str(round(t_N,4)))\n",
    "    print(\"Iterations Nesterov: \" + str(it_N))\n",
    "    print(\"Runtime RestoreX:    \" + str(round(t_R,4)))\n",
    "    print(\"Iterations RestoreX: \" + str(it_R))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize = (15,7))\n",
    "    fig.suptitle(\"$Sigma = $\" + str(sigma) )\n",
    "    ax[0].plot(a_r_R)\n",
    "    ax[0].set_title(\"Alpha coefficients with FISTA\")\n",
    "    ax[0].grid()\n",
    "    ax[0].set_xlabel(\"Iterations\")\n",
    "    ax[1].plot(a_r_N)\n",
    "    ax[1].set_title(\"Alpha coefficients with Forward-Backward\")\n",
    "    ax[1].grid()\n",
    "    ax[1].set_xlabel(\"Iterations\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nit=10000\n",
    "sigma = 0.1\n",
    "alpha_N0,x_N0,CF_N0, a_r_N0, t_N0, it_N0=Nesterov(A,y,sigma,nit)  \n",
    "alpha0,xtilde0,CF0, a_r0, t_R0, it_R0=RestoreX(A,y,sigma,nit)\n",
    "\n",
    "plotter_RvsN(a_r0, a_r_N0, t_N0, it_N0, t_R0, it_R0)\n",
    "\n",
    "sigma = 1\n",
    "alpha_N,x_N,CF_N, a_r_N, t_N, it_N=Nesterov(A,y,sigma,nit)  \n",
    "alpha,xtilde,CF, a_r, t_R, it_R=RestoreX(A,y,sigma,nit)\n",
    "\n",
    "plotter_RvsN(a_r, a_r_N, t_N, it_N, t_R, it_R)\n",
    "\n",
    "sigma = 10\n",
    "alpha_N10,x_N10,CF_N10, a_r_N10, t_N10, it_N10=Nesterov(A,y,sigma,nit)  \n",
    "alpha10,xtilde10,CF10, a_r10, t_R10, it_R10=RestoreX(A,y,sigma,nit)\n",
    "\n",
    "plotter_RvsN(a_r10, a_r_N10, t_N10, it_N10, t_R10, it_R10)\n",
    "\n",
    "sigma = 100\n",
    "alpha_N100,x_N100,CF_N100, a_r_N100, t_N100, it_N100=Nesterov(A,y,sigma,nit)  \n",
    "alpha100,xtilde100,CF100, a_r100, t_R100, it_R100=RestoreX(A,y,sigma,nit)\n",
    "\n",
    "plotter_RvsN(a_r100, a_r_N100, t_N100, it_N100, t_R100, it_R100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que la methode FISTA est meilleure que celle de Forward-Backward Euler et est moins dépendante de $\\sigma$. Pour la valeur de $\\sigma$ trop petite, FISTA ne trouve pas toutes les coefficients $\\alpha$ comme Forward-Backward, en revanche elle converge plus rapidement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nit = 10000\n",
    "n = 500\n",
    "sigma = 1\n",
    "p = 25\n",
    "\n",
    "A1=np.random.randn(p,n) \n",
    "y1=A1.dot(x)        \n",
    "alpha_N, xtilde_N,CF_N, alpha_k_N , t1_N, it1_N =Nesterov(A1,y1,sigma,nit)\n",
    "\n",
    "plotter_2(x_N, xtilde_N, alpha_k_N, p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au-dessus on peut voir qu'avec FISTA on peut reconstruire le signal à partir de $p = 25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13.** Faites un rapide résumé des points qui vous ont semblé les plus importants dans ce TP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au cours de ce TP, on a  exploré qu'on peut avec un nombre limité de mesures reconstruire un signal, representer $x$ par ses coefficients $\\alpha \\in \\mathbb{R}^{2n}$ dans une base de cosinus et diraq. On entraîne un algorithme d'optimisation sur les $\\alpha$ pour trouver les coefficients $\\alpha*$ qui permettent de reconstruire $x$: $x = Psi(\\alpha^*)$.On a comparé principalement deux algorithmes : le Forward-Backward Euler et le FISTA. On a trouvé que FISTA est à la fois plus rapide et meilleur pour la reconstruction du signal malgré que les deux méthodes aboutissent aux mêmes résultats.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
